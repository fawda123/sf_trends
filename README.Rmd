---
title: "README"
output: 
  html_document:
    keep_md: yes
author: "Marcus W. Beck, beck.marcus@epa.gov"
---

### Files

**_data/_** Supporting RData files, usually from data in ignore folder 

* `delt_dat.RData` Processed wq time series data `dwr_wq.RData`, includes all nitrogen analytes and current/active stations in the delta, also includes matched and smoothed flow records from `flocor.RData` results, processed in `dat_proc.R`

* `dwr_wq.RData` time series data of stations in the SF delta from California DWR-EMP (Department of Water Resources, Environmental Monitoring Program) , processed by E. Novick, all stations, analytes from 1975 to present.  Most analytes are measured as concentration, see original spreadsheet for values. 

* `flocor.RData` data.frame of each wq station in `delt_dat.RData` compared with each flow time series in `flow_dat.RData`, lags and correlations for each are shown starting with zero lag back to the previous twelve months, only negative lags are evaluated, i.e, how far back (in months) are the flow time series correlated with nuts, created in `dat_proc.R`

* `flow_dat.RData` time series of daily flow estimates for the delta, input stations from Novick et al (Fig 2) were used, created in `dat_proc.R` 

* `res.RData` sample results for the delta

**_R/_** Supporting R scripts

**_text/_** Summary text of analyses

### Matching flow data with nutrient time series

```{r echo = F, message = F}
library(ggplot2)
library(dplyr)
library(tidyr)
library(WRTDStidal)
library(gridExtra)
source('R/funcs.R')
```

Monthly nutrient samples at each of the active stations in the Delta were compared with daily flow data to identify the most relevant flow estimates for modelling.  Time series were compared using cross-correlation analysis to identify the minimum negative correlation between flow and nutrient concentration and the corresponding lag at which the minimum correlation was observed.  The time series comparisons were based on observed nitrite/nitrate and seasonal means of flow (monthly means of daily records by year).  Flow estimates in Novick et al. were used: Sacramento River plus Yolo bypass (`sacyolo = sac + yolo`), eastern tributaries (`east = csmr + moke + misc`), and San Joaquin River (`sjr`). 

```{r fig.height = 8, fig.width = 12, fig.cap='Cross-correlation analysis of flow records with nitrite/nitrate time series active Delta stations', echo = F}
data(flocor)

ggplot(flocor, aes(x = lag, y = acf, colour = flo, group = flo)) +
  geom_line() + 
  geom_point() + 
  theme_minimal() +
  facet_wrap(~site)
```

```{r, echo = F}
# get the station, lag, and min cor for each site
bests <- group_by(flocor, by = site) %>% 
  filter(acf == min(acf)) %>%
  ungroup %>% 
  select(-by) %>% 
  data.frame

knitr::kable(bests, format = 'markdown', caption = 'Minimum correlations and lags of each nutrient station that corresponded to the flow estimates')
```

The flow record that had the minimum correlation with each nutrient station was then smoothed using a left-centered moving window average that had a window equal in length to the corresponding lag in the above table.  The smoothed flow records were then matched with the monthly nutrient records for the corresponding station.  

```{r fig.height = 8, fig.width = 12, fig.cap='Nitrogen time series versus matched flow records before and after averaging by the maximum lag.', echo = F, message = F, warning = F}
data(delt_dat)

toplo <- select(delt_dat, Site_Code, logq, logqavg, no23) %>% 
  gather('var', 'val', logq:logqavg)

ggplot(toplo, aes(x = val, y = log(1 + no23), group = var, colour = var)) +
  geom_point(size = 0.5, alpha = 0.5) + 
  geom_smooth(method = 'lm', se = F) + 
  scale_x_continuous('log(1 + discharge)') + 
  facet_wrap(~Site_Code, scales = 'free') + 
  theme_minimal()

```

Using the combined flow and nutrient time series, the optimal window widths for each station were identified using functions in the [WRTDStidal](https://github.com/fawda123/WRTDStidal) package.  The `winsrch_optim` function uses model cross-validation to iteratively evaluate multiple half-window widths.  The optimal parameter set is identified based on a minimization of error on a test dataset for multiple subsets of the data.  This method attempts to minimize the tradeoff between over- and under-fitting a model with window widths that are too narrow or too wide, respecively.  The following shows the optimal half-window width combinations identified for each location.  

```{r echo = F}
# get the station, lag, and min cor for each site
data(mods_opt)
totab <- lapply(mods_opt, function(x) x$par) %>% 
  do.call('rbind', .) %>% 
  data.frame(site = row.names(.), .)
names(totab) <- c('site', 'days', 'years', 'flow')

knitr::kable(totab, format = 'markdown', caption = 'Optimal half-window width combinations for each station.  The columns refer to optimal widths for days, years, and flow.  The window width for flow is a propoortion of the total range.', row.names = F, digits = 2)
```

The model predictions and flow-normalized results as annual means are shown below for each of the sites.  The optimal half-window width combinations shown above were used to create each model.  Each color represents a different conditional quantile model fit to the observed time series, i.e., the tenth, fiftieth, and ninetieth percentile distributions.  Note changes in scale, different trends between quantiles, and differences between prediction and flow-normalized results.

```{r, fig.height=8, fig.width = 13, message = F, echo = F, warning = F}
data(mods)

for(i in 1:length(mods)){
  
  lab <- names(mods)[i]
  p <- prdnrmplot(mods[[i]]) +
    ggtitle(lab) + 
    theme_minimal() +
    theme(axis.title = element_blank()) +
    scale_y_continuous(limits = c(0, 1.3))
  
  # get legend
  if(i == 1) pleg <- g_legend(p)
  p <- p + theme(legend.position = 'none')
  
  assign(paste0('p', i), p)
  
}

ylab <- attr(mods[[1]], 'reslab')

grid.arrange(ncol = 2, widths = c(0.02, 1),
  grid::textGrob(ylab, rot = 90), 
  arrangeGrob(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, pleg, ncol = 4)
  )

```

Seasonal variation across all years is viewed using a simple loess (locally estimated) polynomial fit through the model results for the observed data. The points show the observed data at each station, whereas the fit is smoothed through the model predictions (not shown on the plot).

```{r, fig.height = 8, fig.width = 13, message = F, echo = F, warning = F}
data(mods)

for(i in 1:length(mods)){
  
  lab <- names(mods)[i]
  p <- seasplot(mods[[i]], alpha = 0.5, size = 1, lwd = 2) +
    ggtitle(lab) + 
    theme_minimal() +
    theme(axis.title = element_blank()) +
    scale_y_continuous(limits = c(0, 1.6))
    
  # get legend
  if(i == 1) pleg <- g_legend(p)
  p <- p + theme(legend.position = 'none')
  
  assign(paste0('p', i), p)
  
}

ylab <- attr(mods[[1]], 'reslab')

grid.arrange(
  ncol = 1, 
  arrangeGrob(
    ncol = 2, widths = c(0.02, 1),
    grid::textGrob(ylab, rot = 90), 
    arrangeGrob(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, pleg, ncol = 4)
  ), 
  grid::textGrob('Day of year'), 
  heights = c(1, 0.025)
)

```

### To do 

* get detection limits

